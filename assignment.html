<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Allen Hammock" />

<meta name="date" content="2015-04-19" />

<title>Practical Machine Learning Assignment</title>

<script src="assignment_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="assignment_files/bootstrap-3.3.1/css/bootstrap.min.css" rel="stylesheet" />
<script src="assignment_files/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="assignment_files/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="assignment_files/bootstrap-3.3.1/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="assignment_files/highlight/default.css"
      type="text/css" />
<script src="assignment_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">Practical Machine Learning Assignment</h1>
<h4 class="author"><em>Allen Hammock</em></h4>
<h4 class="date"><em>April 19, 2015</em></h4>
</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>The goal of this study is to predict the manner in which a group of subjects performed various weight lifting exercises based on readings collected from devices equiped with an accelerometer, a gyroscope and a magnetometer. A predictive model is built in <code>R</code> code using observations collected from six male participants aged between 20-28 years, with little weight lifting experience.</p>
<p>This report will describe:</p>
<ul>
<li>how we built our model,</li>
<li>how we used cross validation,</li>
<li>what we think the expected out of sample error is, and</li>
<li>why we made the choices we did.</li>
</ul>
</div>
<div id="analysis" class="section level1">
<h1>Analysis</h1>
<div id="reproducibility" class="section level2">
<h2>Reproducibility</h2>
<p>A report on our <code>R</code> environment is presented to assist others in reproducing and validating the models on other similarly equipped systems.</p>
<pre class="r"><code>library(lattice)
library(grid)
library(ggplot2)
library(randomForest)
library(e1071)
library(caret)
library(mlbench)
library(parallel)
library(doMC)

# Set up some variabiles for reproducibility / inspection
use.multicore &lt;- TRUE
seed.value &lt;- 8675309
set.seed(seed.value)
save.environment &lt;- &quot;assignment.RData&quot; 
si &lt;- sessionInfo()
print(si)</code></pre>
<pre><code>## R version 3.1.3 (2015-03-09)
## Platform: x86_64-apple-darwin13.4.0 (64-bit)
## Running under: OS X 10.10.2 (Yosemite)
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  grid      stats     graphics  grDevices utils     datasets 
## [8] methods   base     
## 
## other attached packages:
## [1] doMC_1.3.3          iterators_1.0.7     foreach_1.4.2      
## [4] mlbench_2.1-1       caret_6.0-41        e1071_1.6-4        
## [7] randomForest_4.6-10 ggplot2_0.9.3.1     lattice_0.20-30    
## 
## loaded via a namespace (and not attached):
##  [1] BradleyTerry2_1.0-6 brglm_0.5-9         car_2.0-25         
##  [4] class_7.3-12        codetools_0.2-10    colorspace_1.2-6   
##  [7] digest_0.6.8        drat_0.0.3          evaluate_0.5.5     
## [10] formatR_1.1         gtable_0.1.2        gtools_3.4.1       
## [13] htmltools_0.2.6     knitr_1.9           lme4_1.1-7         
## [16] MASS_7.3-39         Matrix_1.1-5        mgcv_1.8-5         
## [19] minqa_1.2.4         munsell_0.4.2       nlme_3.1-120       
## [22] nloptr_1.0.4        nnet_7.3-9          pbkrtest_0.4-2     
## [25] plyr_1.8.1          proto_0.3-10        quantreg_5.11      
## [28] Rcpp_0.11.5         reshape2_1.4.1      rmarkdown_0.5.1    
## [31] scales_0.2.4        SparseM_1.6         splines_3.1.3      
## [34] stringr_0.6.2       tools_3.1.3         yaml_2.1.13</code></pre>
<pre class="r"><code># Use multiple cores if possible, this speeds up processing 3-4x
if (use.multicore) {
  cores &lt;- detectCores()
  # Save one core for user functions
  if (cores &gt; 2) {
    registerDoMC(cores = ifelse(cores &gt; 3, cores - 2, 2))  
  }
}

# For development only, load previously saved model
# Turn this on only after you&#39;ve used knitr once on this .Rmd file
development &lt;- TRUE
if (development) {
  load(save.environment)
  development &lt;- TRUE
}</code></pre>
</div>
<div id="data-source-citation" class="section level2">
<h2>Data source citation</h2>
<p>The <a href="http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises" title="Human Activity Recognition web site">original source data</a> can be downloaded from the maintained by the <code>Groupware@LES</code> team.</p>
<pre class="r"><code># training &amp; testing: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
modeling.dat &lt;- read.csv(file = &quot;pml-training.csv&quot;) 

# validating: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
validating.dat &lt;- read.csv(file = &quot;pml-testing.csv&quot;) </code></pre>
</div>
<div id="feature-selection-explained" class="section level2">
<h2>Feature selection explained</h2>
<p>In <a href="http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201" title="Qualitative Activity Recognition of Weight Lifting Exercises">the original study</a>, a “Best First” strategy based on backtracking was used to select 17 features in the data set.</p>
<p>In our study, instead we rely only on the original features for which we have complete cases. We remove extracted features such as the calculated kurtosis, mean, and other statistics derived from the original device measurements.</p>
<p>Then, we use the <code>findCorrelation()</code> method from the <code>caret</code> package to remove features that are redundant <a href="http://machinelearningmastery.com/feature-selection-with-the-caret-r-package" title="Feature Selection with the Caret R Package">using a method suggested by Jason Brownlee</a>. By eliminating highly correlated features we hope to reduce the processing time and increase the reliability of the model.</p>
<p>In an <a href="https://github.com/brainvat/PMLAssignment1/commit/29b91b8926d99affbd38532ec26ec71d15bf5653?short_path=9fcaea6#diff-9fcaea68a89fc07bd1251a4b905d600b" title="Github @brainvat early Random Forest model">earlier experiment</a>, we did not test for highly correlated features which greatly increased processing time by ten-fold.</p>
<pre class="r"><code># Keep only the direct measurements, not the derived features like kurtosis or mean
measures &lt;- c(&quot;classe&quot;, 
              grep(x = names(modeling.dat), 
                   pattern = &quot;arm|forearm|dumbell|belt&quot;, 
                   value = TRUE))
exclusions &lt;- grep(x = measures, 
                   pattern = &quot;kurtosis|skewness|max|min|avg|var|stddev|total|amplitude&quot;)
features &lt;- measures[-exclusions]

inTrain &lt;- createDataPartition(y = modeling.dat$classe, p = 0.75, list = FALSE)
training &lt;- modeling.dat[inTrain, features]
testing &lt;- modeling.dat[-inTrain, features]
validating &lt;- validating.dat[, features[-1]]

# Next, remove the highly correlated features
`%ni%` &lt;- Negate(`%in%`)
correlationMatrix &lt;- cor(training[,-1])
highlyCorrelated &lt;- findCorrelation(correlationMatrix, cutoff=0.7)
to.remove &lt;- features[-1][highlyCorrelated]

training &lt;- subset(training, select = names(training) %ni% to.remove)
testing &lt;- subset(testing, select = names(testing) %ni% to.remove)
validating &lt;- subset(validating, select = names(validating) %ni% to.remove)</code></pre>
</div>
<div id="build-the-models-with-cross-validation" class="section level2">
<h2>Build the models with cross-validation</h2>
<p>First, check to make sure we have complete cases, as random forest does not do well with missing values:</p>
<ul>
<li>100% complete cases in training set</li>
<li>100% complete cases in testing set</li>
</ul>
<p>Next, we’ll <strong>center</strong> and <strong>scale</strong> the data using the <code>preProcess</code> directive as we build the model. To avoid over-fitting as much as possible, we employ a 10-fold cross validation procedure using the <code>trainControl()</code> method from the <code>caret</code> package.</p>
<pre class="r"><code># Use 10-fold cross validation
# This takes a long time!
if (!exists(&quot;modFit&quot;)) {
  modFit &lt;- train(classe ~ ., 
                  method = &quot;rf&quot;, 
                  data = training, 
                  preProcess = c(&quot;center&quot;, &quot;scale&quot;), 
                  trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                           number = 10, 
                                           repeats = 3),
                  importance = TRUE)  
}
print(modFit)</code></pre>
<pre><code>## Random Forest 
## 
## 14718 samples
##    23 predictor
##     5 classes: &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39; 
## 
## Pre-processing: centered, scaled 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## 
## Summary of sample sizes: 13247, 13247, 13245, 13245, 13246, 13247, ... 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##    2    0.9857092  0.9819209  0.002308566  0.002922976
##   12    0.9823793  0.9777099  0.004117652  0.005210208
##   23    0.9747467  0.9680538  0.004818513  0.006095248
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<pre class="r"><code>importance &lt;- varImp(modFit, scale = FALSE)
answers.test &lt;- predict(modFit, testing)
cm.test &lt;- confusionMatrix(answers.test, testing$classe)
accuracy.test &lt;- cm.test$overall[c(&quot;Accuracy&quot;)]</code></pre>
</div>
<div id="estimated-out-of-sample-error" class="section level2">
<h2>Estimated out of sample error</h2>
<p>To estimate the out of sample error, we build a confusion matrix, comparing our predicted classes versus the actual classes known in the testing data.</p>
<p>The overall accuracy in our model is <strong>98.55%</strong>. The confusion matrix, below, shows the trained model is able to achieve greater than 97% positive predictive value and over 99% negative predictive value on all classes within a 95% confidence interval of (98.18%, 98.87%).</p>
<p>Looking at the overall importance of each variable in our classification model, we can see that the <code>pitch_belt</code> measurement plays a signficant role in each outcome, especially in Class A where each subject performs the Unilateral Dumbbell Biceps Curl without making any mistakes. Measurements from the magnetometer play a greater role when subjects throw their hips too much to the front (Class E).</p>
<pre class="r"><code>plot(importance)</code></pre>
<p><img src="assignment_files/figure-html/out.of.sample.error-1.png" title="" alt="" width="672" /></p>
<pre class="r"><code>print(cm.test)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1388   10    1    0    0
##          B    6  931   23    0    0
##          C    0    5  824   12    0
##          D    1    1    6  791    2
##          E    0    2    1    1  899
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9855          
##                  95% CI : (0.9818, 0.9887)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9817          
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9950   0.9810   0.9637   0.9838   0.9978
## Specificity            0.9969   0.9927   0.9958   0.9976   0.9990
## Pos Pred Value         0.9921   0.9698   0.9798   0.9875   0.9956
## Neg Pred Value         0.9980   0.9954   0.9924   0.9968   0.9995
## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
## Detection Rate         0.2830   0.1898   0.1680   0.1613   0.1833
## Detection Prevalence   0.2853   0.1958   0.1715   0.1633   0.1841
## Balanced Accuracy      0.9959   0.9869   0.9798   0.9907   0.9984</code></pre>
</div>
</div>
<div id="validation-and-summary" class="section level1">
<h1>Validation and summary</h1>
<p>With high confidence that our model is properly trained and over-fitting is under control, we run the model against the validation data for submission to Coursera. We save our answers in files that can be uploaded per the course submission instructions.</p>
<pre class="r"><code># From coursera submission instructions
# https://class.coursera.org/predmachlearn-013/assignment/view?assignment_id=5
pml_write_files = function(x) {
  n = length(x)
  for(i in 1:n){
    filename = paste0(&quot;problem_id_&quot;,i,&quot;.txt&quot;)
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

answers &lt;- predict(modFit, validating)
pml_write_files(answers)</code></pre>
<div id="environment-inspection" class="section level2">
<h2>Environment inspection</h2>
<p>If you use <code>knitr</code> to build the models yourself, you can inspect the models directly by loading the saved data into your own R environment. Use the following code to open the saved environment:</p>
<pre><code>load(&quot;assignment.RData&quot;)</code></pre>
</div>
<div id="some-extra-fun" class="section level2">
<h2>Some extra fun</h2>
<p>It has been said that one of the big downsides to using machine learning models is that they are not very interpretable. The Random Forests algorithm effectively produces a large number of decision trees which then are used to make predictions.</p>
<p>To see examples of these decision rules, load the saved environment into your session using the code above, then execute the code below. There are a total of 500 trees in the final model.</p>
<pre><code>print.rule(modFit$finalModel, 1)</code></pre>
</div>
<div id="validation-results" class="section level2">
<h2>Validation results</h2>
<p>Now, we’ll show the results of running <code>predict()</code> on our validation set. To protect other Coursera students from violating the Honor Code, we have suppressed the answers in the default output.</p>
<p>To see the final answers, set <code>development &lt;- TRUE</code> in the code above and run the <code>knitr</code> command yourself to see the results inline.</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
