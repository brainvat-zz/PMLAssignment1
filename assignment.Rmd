---
title: "Practical Machine Learning Assignment"
author: "Allen Hammock"
date: "April 19, 2015"
output:
  html_document:
    self_contained: no
---

# Introduction

The goal of this study is to predict the manner in which a group of subjects 
performed various weight lifting exercises based on readings collected from 
devices equiped with an accelerometer, a gyroscope and a magnetometer.  A 
predictive model is built in `R` code using observations collected from six male
participants aged between 20-28 years, with little weight lifting experience.

This report will describe: 

* how we built our model,
* how we used cross validation,
* what we think the expected out of sample error is, and 
* why we made the choices we did.

# Analysis

## Reproducibility

A report on our `R` environment is presented to assist others in reproducing and
validating the models on other similarly equipped systems.

```{r setup.env, message = FALSE, warning = FALSE}
library(lattice)
library(grid)
library(ggplot2)
library(randomForest)
library(e1071)
library(caret)
library(mlbench)
library(parallel)
library(doMC)

# Set up some variabiles for reproducibility / inspection
use.multicore <- TRUE
seed.value <- 8675309
set.seed(seed.value)
save.environment <- "assignment.RData" 
si <- sessionInfo()
print(si)

# Use multiple cores if possible, this speeds up processing 3-4x
if (use.multicore) {
  cores <- detectCores()
  # Save one core for user functions
  if (cores > 2) {
    registerDoMC(cores = ifelse(cores > 3, cores - 2, 2))  
  }
}

# For development only, load previously saved model
# Turn this on only after you've used knitr once on this .Rmd file
development <- FALSE
if (development) {
  load(save.environment)
  development <- TRUE
}
```

## Data source citation

The [original source data][source-data] can be downloaded from the maintained by
the `Groupware@LES` team.  

```{r load.data, cache = TRUE}
# training & testing: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
modeling.dat <- read.csv(file = "pml-training.csv") 

# validating: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
validating.dat <- read.csv(file = "pml-testing.csv") 
```

## Feature selection explained

In [the original study][original-study], a "Best First" strategy based on 
backtracking was used to select 17 features in the data set.  

In our study, instead we rely only on the original features for which we have
complete cases. We remove extracted features such as the calculated kurtosis, 
mean, and other statistics derived from the original device measurements.

Then, we use the `findCorrelation()` method from the `caret` package to remove
features that are redundant 
[using a method suggested by Jason Brownlee][feature-selection]. By eliminating
highly correlated features we hope to reduce the processing time and increase 
the reliability of the model.

In an [earlier experiment][earlier-experiment], we did not test for highly
correlated features which greatly increased processing time by ten-fold.

```{r split.data}
# Keep only the direct measurements, not the derived features like kurtosis or mean
measures <- c("classe", 
              grep(x = names(modeling.dat), 
                   pattern = "arm|forearm|dumbell|belt", 
                   value = TRUE))
exclusions <- grep(x = measures, 
                   pattern = "kurtosis|skewness|max|min|avg|var|stddev|total|amplitude")
features <- measures[-exclusions]

inTrain <- createDataPartition(y = modeling.dat$classe, p = 0.75, list = FALSE)
training <- modeling.dat[inTrain, features]
testing <- modeling.dat[-inTrain, features]
validating <- validating.dat[, features[-1]]

# Next, remove the highly correlated features
`%ni%` <- Negate(`%in%`)
correlationMatrix <- cor(training[,-1])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.7)
to.remove <- features[-1][highlyCorrelated]

training <- subset(training, select = names(training) %ni% to.remove)
testing <- subset(testing, select = names(testing) %ni% to.remove)
validating <- subset(validating, select = names(validating) %ni% to.remove)
```

## Build the models with cross-validation

First, check to make sure we have complete cases, as random forest does not do
well with missing values:

* `r round(100*sum(complete.cases(training))/nrow(training), 0)`% complete cases
in training set
* `r round(100*sum(complete.cases(testing))/nrow(testing), 0)`% complete cases
in testing set

Next, we'll __center__ and __scale__ the data using the `preProcess` directive
as we build the model. To avoid over-fitting as much as possible, we employ a 
10-fold cross validation procedure using the `trainControl()` method from the
`caret` package.

```{r build.model}
# Use 10-fold cross validation
# This takes a long time!
if (!exists("modFit")) {
  modFit <- train(classe ~ ., 
                  method = "rf", 
                  data = training, 
                  preProcess = c("center", "scale"), 
                  trControl = trainControl(method = "repeatedcv", 
                                           number = 10, 
                                           repeats = 3),
                  importance = TRUE)  
}
print(modFit)
importance <- varImp(modFit, scale = FALSE)
answers.test <- predict(modFit, testing)
cm.test <- confusionMatrix(answers.test, testing$classe)
accuracy.test <- cm.test$overall[c("Accuracy")]
```

## Estimated out of sample error

To estimate the out of sample error, we build a confusion matrix, comparing
our predicted classes versus the actual classes known in the testing data.

The overall accuracy in our model is __`r round(100*accuracy.test, 2)`%__. The
confusion matrix, below, shows the trained model is able to achieve greater than 
`r min(round(100*cm.test$byClass[,c("Pos Pred Value")], 0))`% positive predictive 
value and over `r min(round(100*cm.test$byClass[,c("Neg Pred Value")], 0))`% 
negative predictive value on all classes within a 95% confidence interval of
(`r round(100*cm.test$overall[c("AccuracyLower")], 2)`%, 
`r round(100*cm.test$overall[c("AccuracyUpper")], 2)`%).

Looking at the overall importance of each variable in our classification model, 
we can see that the `pitch_belt` measurement plays a signficant role in each 
outcome, especially in Class A where each subject performs the Unilateral 
Dumbbell Biceps Curl without making any mistakes.  Measurements from the
magnetometer play a greater role when subjects throw their hips too much to
the front (Class E).

```{r out.of.sample.error}
plot(importance)
print(cm.test)
```

# Validation and summary

With high confidence that our model is properly trained and over-fitting is 
under control, we run the model against the validation data for submission to 
Coursera.  We save our answers in files that can be uploaded per the course
submission instructions.

```{r output.answers}
# From coursera submission instructions
# https://class.coursera.org/predmachlearn-013/assignment/view?assignment_id=5
pml_write_files = function(x) {
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

answers <- predict(modFit, validating)
pml_write_files(answers)
```

## Environment inspection

If you use `knitr` to build the models yourself, you can inspect the models
directly by loading the saved data into your own R environment.  Use the
following code to open the saved environment:

```
load("`r save.environment`")
```

## Some extra fun

It has been said that one of the big downsides to using machine learning
models is that they are not very interpretable.  The Random Forests
algorithm effectively produces a large number of decision trees which
then are used to make predictions.  

To see examples of these decision rules, load the saved environment into
your session using the code above, then execute the code below.  There are 
a total of `r modFit$finalModel$ntree` trees in the final model.

```
print.rule(modFit$finalModel, 1)
```

```{r print.trees , echo=FALSE}
# Adapted from:
# http://stats.stackexchange.com/questions/41443/how-to-actually-plot-a-sample-tree-from-randomforestgettree

#**************************
#return the rules of a tree
#**************************
getConds<-function(tree){
  #store all conditions into a list
  conds<-list()
  #start by the terminal nodes and find previous conditions
  id.leafs<-which(tree$status==-1)
    j<-0
	  for(i in id.leafs){
		j<-j+1
		prevConds<-prevCond(tree,i)
		conds[[j]]<-prevConds$cond
		while(prevConds$id>1){
		  prevConds<-prevCond(tree,prevConds$id)
		  conds[[j]]<-paste(conds[[j]]," & ",prevConds$cond)
		  if(prevConds$id==1){
			conds[[j]]<-paste(conds[[j]]," => ",tree$prediction[i])
        break()
      }
    }
  }
  return(conds)
}

#**************************
#find the previous conditions in the tree
#**************************
prevCond<-function(tree,i){
  if(i %in% tree$right_daughter){
		id<-which(tree$right_daughter==i)
		cond<-paste(tree$split_var[id],">",tree$split_point[id])
	  }
	  if(i %in% tree$left_daughter){
    id<-which(tree$left_daughter==i)
		cond<-paste(tree$split_var[id],"<",tree$split_point[id])
  }

  return(list(cond=cond,id=id))
}

#remove spaces in a word
collapse<-function(x){
  x<-sub(" ","_",x)
  return(x)
}

print.rule <- function(rf, n) {
  tree<-getTree(rf, k=n, labelVar=TRUE)
  colnames(tree)<-sapply(colnames(tree),collapse)
  rules<-getConds(tree)
  print(rules)
}

```

## Validation results

Now, we'll show the results of running `predict()` on our validation set.  To protect
other Coursera students from violating the Honor Code, we have suppressed the answers
in the default output.

To see the final answers, set `development <- TRUE` in the code above and run the
`knitr` command yourself to see the results inline.

```{r show.answers, echo = FALSE}
if (development) {
  theme.novpadding <- 
    list(layout.heights =
           list(top.padding = 0,
               main = 0,
               main.key.padding = 0,
               key.top = 0,
               xlab.top = 0,
               key.axis.padding = 0,
               axis.top = 0,
               strip = 0,
               #panel = 0,
               axis.panel = 0,
               between = 0,
               axis.bottom = 0,
               xlab = 0,
               axis.xlab.padding = 0,
               xlab.key.padding = 0,
               key.bottom = 0,
               sub = 0,
               key.sub.padding = 0,
               bottom.padding = 0),
        layout.widths =
          list(left.padding = 0,
               key.left = 0,
               key.ylab.padding = 0,
               ylab = 0,
               ylab.axis.padding = 0,
               axis.left = 0,
               axis.panel = 0,
               strip.left = 0,
               #panel = 0,
               between = 0,
               axis.right = 0,
               axis.key.padding = 0,
               ylab.right = 0,
               key.right = 0,
               right.padding = 0),
        axis.line =
          list(col = NA, 
               lty = 1, 
               lwd = 1))

  trellis.par.set(theme.novpadding)
  p1 <- xyplot(runif(1500) ~ 1:250, 
         xlab = NULL, ylab = NULL, 
         scales=list(axs = 'i', draw = FALSE), 
         pch = 10, aspect = 0.25, col = "gray64", layout = c(1,1))

  print(p1)
  trellis.focus("toplevel", 1, 1)
  panel.text(0.5, 0.5, paste(answers, collapse = " "), cex = 1.6, font = 2, col = "gray48")
  trellis.unfocus()  
}
```

```{r save.environment , echo=FALSE}
if (!development) {
  save.image(save.environment)
}
```

[source-data]: http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises "Human Activity Recognition web site"
[feature-selection]: http://machinelearningmastery.com/feature-selection-with-the-caret-r-package "Feature Selection with the Caret R Package"
[original-study]: http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201 "Qualitative Activity Recognition of Weight Lifting Exercises"
[earlier-experiment]: https://github.com/brainvat/PMLAssignment1/commit/29b91b8926d99affbd38532ec26ec71d15bf5653?short_path=9fcaea6#diff-9fcaea68a89fc07bd1251a4b905d600b "Github @brainvat early Random Forest model"
